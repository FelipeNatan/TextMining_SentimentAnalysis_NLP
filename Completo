#PARTE 1 
#Pacotes utilizados
pacotes <- c("tidytext","ggplot2","dplyr","tibble","gutenbergr","wordcloud","stringr","SnowballC","widyr","janeaustenr")

if(sum(as.numeric(!pacotes %in% installed.packages())) != 0){
  instalador <- pacotes[!pacotes %in% installed.packages()]
  for(i in 1:length(instalador)) {
    install.packages(instalador, dependencies = T)
    break()}
  sapply(pacotes, require, character = T) 
} else {
  sapply(pacotes, require, character = T) 
}

library("dplyr")
library("tidytext")
library("ggplot2")
library("tibble")

#Relembrar dplyr
st<-starwars

filtro<-st %>% filter(species=="Droid")
select<-st %>% select (name, mass)
altura<-st %>% mutate (altura = height/100)

head(filtro)
head(select)
head(altura)

#SCRIPT 1
#Texto - the black cat
text<-c("From my infancy I was noted for the docility and humanity of my disposition.",
        "My tenderness of heart was even so conspicuous as to make me the jest of my companions.",
        "There is something in the unselfish and self-sacrificing love of a brute, which goes directly to the heart of him who has had frequent occasion to test the paltry friendship and gossamer fidelity of mere Man.")


#Transforma em tibble (data frame)
text_df<-tibble(line=1:3,
                text=text)

text_df

#Tokeninza texto (quebra em palavras e retorna em texto)
df<- text_df %>% unnest_tokens(word,     ##pode ser desde uma letra até uma frase se tiver sentido
                               text)     ##ja convertido em minuscula e sem pontuacao

#Pré-processamento
##Remoção de stop words
??stop_words
stop_words    ##lista com palavras do pacote tidytext

df_sem_stop_words <- df %>% anti_join(stop_words)

##Bag os words (busca palavras mais comuns)
conta<-df_sem_stop_words %>% count(word)

##Grafico 
df_sem_stop_words %>% 
  count(word,sort=T) %>% 
  mutate(word = reorder(word,n)) %>% 
  ggplot(aes(n,word))+
  geom_col()






#SCRIPT 2
library("gutenbergr")
library("dplyr")
library("tidytext")
library("wordcloud")
library("stringr")
library("SnowballC")

textos<-gutenberg_metadata

#Usar livros Moby Dick e Peter Pan
livros<-gutenberg_download(c(15,16))

#Retirar números ou qualquer outro elemento
nums<-livros %>% filter(str_detect(text,"^[0-9]")) %>% select(text) #filtra na coluna text todas as observações que começam com 0 a 9

livros<-livros %>% anti_join(nums,by="text")

#Deletar stop words e tokenizar base
livros<-livros %>% unnest_tokens(word,text) %>%     #faz token por palavras com base na coluna texto 
                    anti_join(stop_words)           #anti_join na base stop_words


#Moby Dick
moby<-livros %>% filter(gutenberg_id== 15) %>% 
                 count(word, sort=T)              #sort=T coloca em ordem decrescente

#Cria word cloud
##define paleta de cores para a wordcloud
pal<-brewer.pal(8,"Dark2") #8 cores de Dark2

##word cloud
moby %>% with(wordcloud(words=word,
                        freq=n,
                        random.order=F,
                        max.words=70,          #as 'x' palavras que mais ocorrem
                        colors=pal))

#Peter Pan
peter<-livros %>% filter(gutenberg_id == 16) %>% 
                  count(word,sort=T)

pal<-brewer.pal(8,"Accent")

peter %>% with(wordcloud(word,n,random.order = F,max.words = 30,colors = pal))

##stemming
peter_stem<-peter %>% mutate(stem=wordStem(word)) #faz o stem com base na lista do pocote snowball

peter_stem_count<- livros %>% filter(gutenberg_id==16) %>% 
                              mutate(stem=wordStem(word)) %>% 
                              count(stem, sort=T)






#SCRIPT 3
library("gutenbergr")
library("dplyr")
library("tidytext")
library("wordcloud")
library("stringr")
library("SnowballC")
textos<-gutenberg_metadata

textos$language=='pt'  #portugues

livro<-textos %>% filter(gutenberg_id==33752) %>% 
                  gutenberg_download()

#Corrigindo problemas de caracteres especiais
Encoding(livro$text)<-"ASCII"

#Padronizando texto para padrão computacional (sem caracteres especiais)
for (i in 1:nrow(livro))
{
  livro$text[i] <- iconv(livro$text[i], to = "ASCII//TRANSLIT")
}

#Tokeniza e faz stop words portugueses
livro<-livro %>% unnest_tokens(word,text) %>% 
                 anti_join(get_stopwords('pt'))

#Contagem de palavras
livro_count<-livro %>% select(word) %>% 
                       count(word,sort=T)

#Stemming
livro_stem <-  livro %>%  mutate(stem = wordStem(word)) 

#Contagem de stems
livro_cont <- livro_stem %>% select(word) %>% count(word, sort = TRUE)

#Grafico wordcloud
pal <- brewer.pal(8,"Dark2")
livro_cont %>% with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=pal))






#SCRIPT 4
library("gutenbergr")
library("dplyr")
library("tidytext")
library("wordcloud")
library("stringr")
library("SnowballC")

textos <- gutenberg_metadata

#Vamos usar última ceia do dr fausto
livro <- gutenberg_download(c(35982))

#Mudamos o encoding para ver os acentos
#Discutir o que é encoding: https://blog.caelum.com.br/entendendo-unicode-e-os-character-encodings/amp/
Encoding(livro$text) <- "ASCII"

#vamos retirar os acentos
for (i in 1:nrow(livro))
{
  livro$text[i] <- iconv(livro$text[i], to = "ASCII//TRANSLIT")
}

#Unnest tokens - vamos fazer n gramas de 2 = bigrama
livro <- livro %>%  unnest_tokens(word, text, token = "ngrams", n = 2) 

#Contagem
cont<-livro %>% count(word,sort=T)

#Correlação entre palavras
library("widyr")
library("janeaustenr")

austen_section_words <- austen_books() %>%
  filter(book == "Pride & Prejudice") %>%
  mutate(section = row_number() %/% 10) %>%
  filter(section > 0) %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% stop_words$word)

word_pairs <- austen_section_words %>%
  pairwise_count(word, section, sort = TRUE)

word_cors <- austen_section_words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, section, sort = TRUE)


#PARTE 2
#Script 1

library("gutenbergr")
library("dplyr")
library("tidytext")
library("ggplot2")

#Textos do pacote gutenberg
textos<-gutenberg_metadata
textos

#Importa livros Othllo e Contos do Norte
livros<-gutenberg_download(c(28526,28691))
livros

#Faz unnest tokens 
book_words<- livros %>% 
            unnest_tokens(word, text) %>% 
            count(gutenberg_id, word,sort=T)
book_words

#TF-IDF
books_tf_idf<-book_words %>% bind_tf_idf(word,gutenberg_id,n)
books_tf_idf

#Palavras mais importantes por texto
othello_tf_idf<-books_tf_idf %>% filter(gutenberg_id==28526)
othello_tf_idf

contos_tf_idf<-books_tf_idf %>% filter (gutenberg_id == 28691)
contos_tf_idf

#Gráfico
books_graph <- books_tf_idf %>% 
  group_by(gutenberg_id) %>% 
  slice_max(tf_idf, n = 15) %>% 
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) 

books_graph %>% ggplot(aes(tf_idf, word, fill = gutenberg_id)) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  facet_wrap(~gutenberg_id, ncol = 2, scales = "free")





#Script 2
library("gutenbergr")
library("dplyr")
library("tidytext")
library("ggplot2")
library("lexiconPT")
library("janeaustenr")
library("stringr")
library("tidyr")
library("wordcloud")

#Tipos de sentimentos
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")

#Análise de sentimento (inner join)
tidy_books<-austen_books() %>%
            group_by(book) %>%
            unnest_tokens(word,text)

#Palavras mais comuns de prazer para análise de sentimentos
nrc_joy<-get_sentiments("nrc") %>% 
        filter(sentiment == "joy")

#Realizar inner join com o livro Emma para entender sentimentos
book<-tidy_books %>% filter(book == "Emma") %>%  
                     inner_join(nrc_joy) %>% 
                     count(word, sort=TRUE)  

#Usando sistema bing para calcular o sentimento em polaridade
jane_austen_sentiment<- tidy_books %>% inner_join(get_sentiments("bing"))
jane_austen_sentiment

persuasion <- jane_austen_sentiment %>% filter (book == "Persuasion")

persuasion <- persuasion %>% mutate (net_sentiment = ifelse(sentiment =="negative",-1,1))

sum(persuasion$net_sentiment)

#Word cloud
library("reshape2")

mansfield <- tidy_books %>% filter(book == "Mansfield Park")

mansfield %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("red", "green"),
                   scale=c(1,.5),
                   max.words = 100)





#Script 3
library("readxl")
library("tm")
library("wordcloud")
library("e1071")
library("gmodels")
library("SnowballC")
library("caret")
library("dplyr")

#Extraindo dados
twitter <- read.csv2("twitter_validation.csv",sep = ",",header = FALSE)

twitter <- twitter %>% select(V3,V4) %>% filter(V3 == c('Positive','Negative'))

colnames(twitter) <- c("sentiment","text")

twitter

#Limpando dados
#Train set
corpus = VCorpus(VectorSource(twitter$text))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers) 
corpus = tm_map(corpus, removePunctuation) 
corpus = tm_map(corpus, removeWords, stopwords("english")) 
corpus = tm_map(corpus, stemDocument) 
corpus = tm_map(corpus, stripWhitespace) 
as.character(corpus[[1]])

#Document Term Matrix
dtm=DocumentTermMatrix(corpus)
inspect(dtm)
dim(dtm)
dtm=removeSparseTerms(dtm,0.999)
inspect(dtm)
dim(dtm)

convert <- function(x) 
{
  y <- ifelse(x > 0, 1,0)
  y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
  y
}  

datanaive = apply(dtm, 2, convert)

dataset = as.data.frame(as.matrix(datanaive))

dataset$Class = factor(twitter$sentiment)

dataset


#Train Test Split
set.seed(31)
split = sample(2,nrow(dataset),prob = c(0.75,0.25),replace = TRUE)
train_set = dataset[split == 1,]
test_set = dataset[split == 2,] 

prop.table(table(train_set$Class))
prop.table(table(test_set$Class))

classifier_nb <- naiveBayes(train_set[1:1439], train_set$Class)

#Predição do Test Set
nb_pred = predict(classifier_nb, type = 'class', newdata =  test_set[1:1439])
confusionMatrix(nb_pred,test_set$Class)
